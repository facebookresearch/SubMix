{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03031be1",
   "metadata": {},
   "source": [
    "### Example code for using SubMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, start off by getting your model ensemble. \n",
    "#You can use X to download the wikitext-103 dataset.\n",
    "#If you don't want to train your own, you can access an 8-fold partition from Y.\n",
    "#You'll also need a public LM from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1028adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the configurations here\n",
    "B = 1024 #query budget\n",
    "eps = 2 #target epsilon (information leakage)\n",
    "alpha = 2 #alpha for Renyi Divergence\n",
    "seqlen = 512 #sequence length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "public_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "import os\n",
    "#download 8 fold ensembles for wikitext-103\n",
    "s3path = 'https://dl.fbaipublicfiles.com/submix/ensembles'\n",
    "s3path += '/gpt2_wikitext_finetune_8fold'\n",
    "ensemble = []\n",
    "for i in range(8):\n",
    "    print(f'{s3path}/{i}')\n",
    "    os.system(f'wget {s3path}/{i}')\n",
    "    ensemble.append(torch.load(f'{i}').to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from submix import SubMix\n",
    "\n",
    "SM = SubMix('cpu', B, eps, public_model, ensemble, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utility functions and classes defined here\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def load_wikitext():\n",
    "    corpus = dict()\n",
    "    for dset in [\"valid\", \"train\", \"test\"]:\n",
    "        corpus[dset] = torch.load(f\"wikitext-103-{dset}-corpus.pt\")\n",
    "    return corpus\n",
    "\n",
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, corpus, seqlen):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.seqlen = seqlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.corpus) / self.seqlen)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        idx = item * self.seqlen\n",
    "        return self.corpus[idx : idx + self.seqlen]\n",
    "\n",
    "def XHeval(lm_logits, labels):\n",
    "    #computes cross entropy\n",
    "    shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    # Flatten the tokens\n",
    "    XH = nn.CrossEntropyLoss()\n",
    "    return XH(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).item()\n",
    "    \n",
    "def test(\n",
    "    SM,\n",
    "    device,\n",
    "    loader,\n",
    "    max_iters\n",
    "):\n",
    "    for model in SM.LMs:\n",
    "        model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm.tqdm(loader)):\n",
    "            data = data.to(device)\n",
    "            if isinstance(data,str):\n",
    "                data = torch.tensor(tokenizer.encode(x)).to(device)\n",
    "            L, P = SM.compute_logits_at_context(data)\n",
    "            L_submix = []\n",
    "            for j in range(len(P[0])):\n",
    "                prob_submix = SM.query( [p[j] for p in P] )\n",
    "                L_submix.append(torch.log(prob_submix))\n",
    "            L_submix = torch.stack(L_submix)\n",
    "            losses.append(XHeval(L_submix, data))\n",
    "            #losses.append(loss.item())\n",
    "            if i >= max_iters:\n",
    "                break\n",
    "        print(f\"Val Loss: {np.mean(losses):.4f} \")\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "corpus = load_wikitext() #load data\n",
    "val_loader = DataLoader(CorpusDataset(corpus['valid'], seqlen)) #define data loader\n",
    "L = test(SM, 'cpu', val_loader, B/512) # compute PPL\n",
    "#NOTE: The query budget is on a per token basis, not a per sequence basis\n",
    "#therefore, we recommend running test a large number of times and taking \n",
    "#an average value in order to get a good estimate of the PPL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
